{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FpU62F2Ii5MT"
   },
   "source": [
    "## Import all necessary libraries \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "faqQ6viXh5En"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "import cv2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9D09D0mMjKxC"
   },
   "source": [
    "## Describe input\n",
    "\n",
    "This network is a simple feedforward neural without with 10 inputs, a first hidden layer with 8 nodes, a second hidden layer with 4 nodes, and a final output layer used for regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KYAk9vYMiC8m"
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(10,))\n",
    "x = Dense(8, activation=\"relu\")(inputs)\n",
    "x = Dense(4, activation=\"relu\")(x)\n",
    "x = Dense(1, activation=\"linear\")(x)\n",
    "model = Model(inputs, x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L88aeRKFjWsU"
   },
   "source": [
    "## Describe multiple inputs \n",
    "\n",
    "We would describe two inputs in our models \n",
    "A  with 32 dim \n",
    "B with 128 dim \n",
    "This is an overview of using multiple inputs to give us a final output. <br>\n",
    "We describe the 2 branches and then combine them using the concatenate method after which we apply a FC layer which outputs a single value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyI7qA4Yh8Ng"
   },
   "outputs": [],
   "source": [
    "# define two sets of inputs\n",
    "inputA = Input(shape=(32,))\n",
    "inputB = Input(shape=(128,))\n",
    "\n",
    "\n",
    "# the first branch operates on the first input\n",
    "x = Dense(8, activation=\"relu\")(inputA)\n",
    "x = Dense(4, activation=\"relu\")(x)\n",
    "x = Model(inputs=inputA, outputs=x)\n",
    "\n",
    "\n",
    "# the second branch operates on the second input\n",
    "y = Dense(64, activation=\"relu\")(inputB)\n",
    "y = Dense(32, activation=\"relu\")(y)\n",
    "y = Dense(4, activation=\"relu\")(y)\n",
    "y = Model(inputs=inputB, outputs=y)\n",
    "\n",
    "\n",
    "# combine the output of both branches \n",
    "# combined = concatenate([x.output, y.output])\n",
    "combined = layers.concatenate([x.output, y.output])\n",
    "\n",
    "\n",
    "# apply a Fully Connected layer and a linear regression of the \n",
    "# combined outputs\n",
    "z = Dense(2, activation=\"relu\")(combined)\n",
    "z = Dense(1, activation=\"linear\")(z)\n",
    "\n",
    "\n",
    "# our model will accept the inputs of the two branches and\n",
    "# then output a single value\n",
    "model = Model(inputs=[x.input, y.input], outputs=z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8GdlB4So8YO"
   },
   "source": [
    "## Loading the numerical and categorical data\n",
    "\n",
    "Load data with columns \n",
    "\n",
    "Find unique zip codes and corresponding count, eliminating counts less than 25. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LHNIJXYRioYc"
   },
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "# import cv2\n",
    "import os\n",
    "\n",
    "# Define a function to load the house attributes \n",
    "def load_house_attributes(inputPath):\n",
    "\t# initialize the list of column names in the CSV file and then\n",
    "\t# load it using Pandas\n",
    "\tcols = [\"bedrooms\", \"bathrooms\", \"area\", \"zipcode\", \"price\"]\n",
    "\tdf = pd.read_csv(inputPath, sep=\" \", header=None, names=cols)\n",
    "    \n",
    "\t# determine (1) the unique zip codes and (2) the number of data\n",
    "\t# points with each zip code\n",
    "    \n",
    "\tzipcodes = df[\"zipcode\"].value_counts().keys().tolist()\n",
    "\tcounts = df[\"zipcode\"].value_counts().tolist()\n",
    "    \n",
    "    \n",
    "\t# loop over each of the unique zip codes and their corresponding\n",
    "\t# count\n",
    "\tfor (zipcode, count) in zip(zipcodes, counts):\n",
    "\t\t# the zip code counts for our housing dataset is *extremely*\n",
    "\t\t# unbalanced (some only having 1 or 2 houses per zip code)\n",
    "\t\t# so let's sanitize our data by removing any houses with less\n",
    "\t\t# than 25 houses per zip code\n",
    "\t\tif count < 25:\n",
    "            # Give seperate column \n",
    "\t\t\tidxs = df[df[\"zipcode\"] == zipcode].index\n",
    "            # drop sepearate column \n",
    "\t\t\tdf.drop(idxs, inplace=True)\n",
    "\t# return the data frame\n",
    "\treturn df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wldVm2sro8YS"
   },
   "source": [
    "## Define house process function \n",
    "We apply min-max scaling on continous features and one hot encoding on categorical features, after which we concatenate continous and categorical features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QxrBB7tao8YS"
   },
   "outputs": [],
   "source": [
    "def process_house_attributes(df, train, test):\n",
    "\t# initialize the column names of the continuous data\n",
    "\tcontinuous = [\"bedrooms\", \"bathrooms\", \"area\"]\n",
    "\t# performin min-max scaling each continuous feature column to\n",
    "\t# the range [0, 1]\n",
    "\tcs = MinMaxScaler()\n",
    "\ttrainContinuous = cs.fit_transform(train[continuous])\n",
    "\ttestContinuous = cs.transform(test[continuous])\n",
    "\t# one-hot encode the zip code categorical data (by definition of\n",
    "\t# one-hot encoding, all output features are now in the range [0, 1])\n",
    "\tzipBinarizer = LabelBinarizer().fit(df[\"zipcode\"])\n",
    "\ttrainCategorical = zipBinarizer.transform(train[\"zipcode\"])\n",
    "\ttestCategorical = zipBinarizer.transform(test[\"zipcode\"])\n",
    "\t# construct our training and testing data points by concatenating\n",
    "\t# the categorical features with the continuous features\n",
    "    # np.hstack stack arrays in sequence horizontally \n",
    "\ttrainX = np.hstack([trainCategorical, trainContinuous])\n",
    "\ttestX = np.hstack([testCategorical, testContinuous])\n",
    "\t# return the concatenated training and testing data\n",
    "\treturn (trainX, testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zWyGHNHUo8YV"
   },
   "source": [
    "### load_house_images function has three goals:\n",
    "\n",
    "Load all photos from the House Prices dataset. Recall that we have four photos per house (Figure 6).\n",
    "Generate a single montage image from the four photos. The montage will always be arranged as you see in the figure.\n",
    "Append all of these home montages to a list/array and return to the calling function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DaMcxbNao8YW"
   },
   "outputs": [],
   "source": [
    "def load_house_images(df, inputPath):\n",
    "    #initialize image array \n",
    "    images = []\n",
    "    # loop over the house indexes \n",
    "    for i in df.index.values: \n",
    "        #each house has four images, find and sort the file paths \n",
    "        # ensure the images are in the same order \n",
    "        basePath = os.path.sep.join([inputPath, \"{}_*\".format(i + 1)])\n",
    "        # glob is to search for files recursively\n",
    "        # operating on a directory and its contents, including the contents of any subdirectories\".\n",
    "        housePaths = sorted(list(glob.glob(basePath)))\n",
    "        # initialize our list of input images along with the output image\n",
    "        # after *combining* the four input images\n",
    "        inputImages = []\n",
    "        outputImage = np.zeros((64, 64, 3), dtype=\"uint8\")\n",
    "        # loop over the input house paths\n",
    "        for housePath in housePaths:\n",
    "            # load the input image, resize it to be 32 32, and then\n",
    "            # update the list of input images\n",
    "            image = cv2.imread(housePath)\n",
    "            image = cv2.resize(image, (32, 32))\n",
    "            inputImages.append(image)\n",
    "        # tile the four input images in the output image such the first\n",
    "        # image goes in the top-right corner, the second image in the\n",
    "        # top-left corner, the third image in the bottom-right corner,\n",
    "        # and the final image in the bottom-left corner\n",
    "        outputImage[0:32, 0:32] = inputImages[0]\n",
    "        outputImage[0:32, 32:64] = inputImages[1]\n",
    "        outputImage[32:64, 32:64] = inputImages[2]\n",
    "        outputImage[32:64, 0:32] = inputImages[3]\n",
    "        # add the tiled image to our set of images the network will be\n",
    "        # trained on\n",
    "        images.append(outputImage)\n",
    "        # return our set of images\n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1tKaszIo8YZ"
   },
   "source": [
    "## Defining our Multi-layer Perceptron (MLP) and Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aFmRdE8eo8YZ"
   },
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wbFJVQDmo8Yc"
   },
   "outputs": [],
   "source": [
    "def create_mlp(dim, regress=False):\n",
    "\t# define our MLP network\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(8, input_dim=dim, activation=\"relu\"))\n",
    "\tmodel.add(Dense(4, activation=\"relu\"))\n",
    "\t# check to see if the regression node should be added\n",
    "\tif regress:\n",
    "\t\tmodel.add(Dense(1, activation=\"linear\"))\n",
    "\t# return our model\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "niNpluaFo8Yl"
   },
   "source": [
    "## create_cnn function\n",
    "create_cnn function handles the image data and accepts five parameters:\n",
    "\n",
    "width : The width of the input images in pixels.\n",
    "<br>\n",
    "height : How many pixels tall the input images are.<br>\n",
    "depth : The number of channels in our input images. For RGB color images, it is three.<br>\n",
    "filters : A tuple of progressively larger filters so that our network can learn more discriminate features.<br>\n",
    "regress : A boolean indicating whether or not a fully-connected linear activation layer will be appended to the CNN for regression purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0HiU3ZLdo8Ym"
   },
   "outputs": [],
   "source": [
    "def create_cnn(width, height, depth, filters=(16, 32, 64), regress=False):\n",
    "\t# initialize the input shape and channel dimension, assuming\n",
    "\t# TensorFlow/channels-last ordering\n",
    "\tinputShape = (height, width, depth)\n",
    "\tchanDim = -1\n",
    "\t# define the model input\n",
    "\tinputs = Input(shape=inputShape)\n",
    "\t# loop over the number of filters\n",
    "\tfor (i, f) in enumerate(filters):\n",
    "\t\t# if this is the first CONV layer then set the input\n",
    "\t\t# appropriately\n",
    "\t\tif i == 0:\n",
    "\t\t\tx = inputs\n",
    "\t\t# CONV => RELU => BN => POOL\n",
    "\t\tx = Conv2D(f, (3, 3), padding=\"same\")(x)\n",
    "\t\tx = Activation(\"relu\")(x)\n",
    "\t\tx = BatchNormalization(axis=chanDim)(x)\n",
    "\t\tx = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "        # flatten the volume, then FC => RELU => BN => DROPOUT\n",
    "\tx = Flatten()(x)\n",
    "\tx = Dense(16)(x)\n",
    "\tx = Activation(\"relu\")(x)\n",
    "\tx = BatchNormalization(axis=chanDim)(x)\n",
    "    # Drop out helps prevent overfitting by randomly eliminating inputs to avoid reliance\n",
    "    # on any one feature. This way we spread the weight (weight shrinking)\n",
    "\tx = Dropout(0.5)(x)\n",
    "\t# apply another FC layer, this one to match the number of nodes\n",
    "\t# coming out of the MLP\n",
    "\tx = Dense(4)(x)\n",
    "\tx = Activation(\"relu\")(x)\n",
    "\t# check to see if the regression node should be added\n",
    "\tif regress:\n",
    "\t\tx = Dense(1, activation=\"linear\")(x)\n",
    "\t# construct the CNN\n",
    "\tmodel = Model(inputs, x)\n",
    "\treturn model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jSY_O4HJo8Yo"
   },
   "source": [
    "## Multiple inputs with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "fkU9npLao8Yp"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/user/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/user/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/user/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/user/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/user/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/user/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/user/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/user/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/user/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/user/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/user/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "\n",
    "# from pyimagesearch import datasets\n",
    "# from pyimagesearch import models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import concatenate\n",
    "import numpy as np\n",
    "import argparse\n",
    "import locale\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17WKbBAxo8Yr"
   },
   "source": [
    "## Parser to Dataset \n",
    " args Handles parsing command line arguments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FzRFyt5No8Ys"
   },
   "outputs": [],
   "source": [
    "# construct the argument parser and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-d\", \"--dataset\", type=str, required=True,\n",
    "\thelp=\"path to input dataset of house images\")\n",
    "args = vars(ap.parse_args())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEVODl1jo8Yw"
   },
   "source": [
    "## Load Dataset\n",
    "load numerical/categorical data and image data with our earlier defined functions <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gz0lSnzvo8Yx"
   },
   "outputs": [],
   "source": [
    "# construct the path to the input .txt file that contains information\n",
    "# on each house in the dataset and then load the dataset\n",
    "print(\"[INFO] loading house attributes...\")\n",
    "inputPath = os.path.sep.join([args[\"dataset\"], \"HousesInfo.txt\"])\n",
    "df = datasets.load_house_attributes(inputPath)\n",
    "# load the house images and then scale the pixel intensities to the\n",
    "# range [0, 1]\n",
    "print(\"[INFO] loading house images...\")\n",
    "images = datasets.load_house_images(df, args[\"dataset\"])\n",
    "images = images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eOnOmhwVo8Yz"
   },
   "source": [
    "## Partition Dataset into Training and Testing Data \n",
    "perform min-max scaling on continuous features and one-hot encoding on categorical features with our earlier defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k3mqRgUIo8Y0"
   },
   "outputs": [],
   "source": [
    "# partition the data into training and testing splits using 75% of\n",
    "# the data for training and the remaining 25% for testing\n",
    "print(\"[INFO] processing data...\")\n",
    "split = train_test_split(df, images, test_size=0.25, random_state=42)\n",
    "(trainAttrX, testAttrX, trainImagesX, testImagesX) = split\n",
    "# find the largest house price in the training set and use it to\n",
    "# scale our house prices to the range [0, 1] (will lead to better\n",
    "# training and convergence)\n",
    "maxPrice = trainAttrX[\"price\"].max()\n",
    "trainY = trainAttrX[\"price\"] / maxPrice\n",
    "testY = testAttrX[\"price\"] / maxPrice\n",
    "# process the house attributes data by performing min-max scaling\n",
    "# on continuous features, one-hot encoding on categorical features,\n",
    "# and then finally concatenating them together\n",
    "(trainAttrX, testAttrX) = datasets.process_house_attributes(df,\n",
    "\ttrainAttrX, testAttrX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lv9DR2A6o8Y2"
   },
   "source": [
    "## Concatenate both Inputs \n",
    "Create and concatenate both models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nEt8SPiXo8Y3"
   },
   "outputs": [],
   "source": [
    "# create the MLP and CNN models\n",
    "mlp = models.create_mlp(trainAttrX.shape[1], regress=False)\n",
    "cnn = models.create_cnn(64, 64, 3, regress=False)\n",
    "# create the input to our final set of layers as the *output* of both\n",
    "# the MLP and CNN\n",
    "combinedInput = concatenate([mlp.output, cnn.output])\n",
    "# our final FC layer head will have two dense layers, the final one\n",
    "# being our regression head\n",
    "x = Dense(4, activation=\"relu\")(combinedInput)\n",
    "x = Dense(1, activation=\"linear\")(x)\n",
    "# our final model will accept categorical/numerical data on the MLP\n",
    "# input and images on the CNN input, outputting a single value (the\n",
    "# predicted price of the house)\n",
    "model = Model(inputs=[mlp.input, cnn.input], outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NlJCWaAdo8Y5"
   },
   "source": [
    "## Combine and train newly formed models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kJAnaxi9o8Y6"
   },
   "outputs": [],
   "source": [
    "# compile the model using mean absolute percentage error as our loss,\n",
    "# implying that we seek to minimize the absolute percentage difference\n",
    "# between our price *predictions* and the *actual prices*\n",
    "opt = Adam(lr=1e-3, decay=1e-3 / 200)\n",
    "model.compile(loss=\"mean_absolute_percentage_error\", optimizer=opt)\n",
    "# train the model\n",
    "print(\"[INFO] training model...\")\n",
    "model.fit(\n",
    "\tx=[trainAttrX, trainImagesX], y=trainY,\n",
    "\tvalidation_data=([testAttrX, testImagesX], testY),\n",
    "\tepochs=200, batch_size=8)\n",
    "# make predictions on the testing data\n",
    "print(\"[INFO] predicting house prices...\")\n",
    "preds = model.predict([testAttrX, testImagesX])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WaJewSk5o8ZA"
   },
   "source": [
    "## Checking the Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IqINGh2mo8ZB"
   },
   "outputs": [],
   "source": [
    "# compute the difference between the *predicted* house prices and the\n",
    "# *actual* house prices, then compute the percentage difference and\n",
    "# the absolute percentage difference\n",
    "diff = preds.flatten() - testY\n",
    "percentDiff = (diff / testY) * 100\n",
    "absPercentDiff = np.abs(percentDiff)\n",
    "# compute the mean and standard deviation of the absolute percentage\n",
    "# difference\n",
    "mean = np.mean(absPercentDiff)\n",
    "std = np.std(absPercentDiff)\n",
    "# finally, show some statistics on our model\n",
    "locale.setlocale(locale.LC_ALL, \"en_US.UTF-8\")\n",
    "print(\"[INFO] avg. house price: {}, std house price: {}\".format(\n",
    "\tlocale.currency(df[\"price\"].mean(), grouping=True),\n",
    "\tlocale.currency(df[\"price\"].std(), grouping=True)))\n",
    "print(\"[INFO] mean: {:.2f}%, std: {:.2f}%\".format(mean, std))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "multiple_input _from_pyimage.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
